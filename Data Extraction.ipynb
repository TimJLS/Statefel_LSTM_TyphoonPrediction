{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import lxml.etree as etree\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '1987-2016.kml'\n",
    "# read file\n",
    "ty_name=[]\n",
    "intensity=[]\n",
    "mslp=[]\n",
    "max_int=[]\n",
    "min_slp=[]\n",
    "timesteps=[]\n",
    "lon=[]\n",
    "lat=[]\n",
    "parser = etree.XMLParser(strip_cdata=False)\n",
    "tree = etree.parse(file_path, parser=parser)\n",
    "#print(etree.tostring(tree, xml_declaration=True))\n",
    "#print(tree.xpath('//kml:Placemark/kml:TimeSpan/text()', namespaces={\"kml\":\"http://www.opengis.net/kml/2.2\"}))\n",
    "nmsp = '{http://www.opengis.net/kml/2.2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### walk through history typhoon file ###\n",
    "for fd in tree.iterfind('/{0}Document/{0}Folder/{0}Folder/{0}Folder'.format(nmsp)):\n",
    "    temp = fd.find('{0}Placemark/{0}name'.format(nmsp)).text\n",
    "    ty_name.append(temp)\n",
    "    count=0\n",
    "    ## feature extraction ##\n",
    "    for dscp in fd.iterfind('{0}Placemark'.format(nmsp)):\n",
    "        #print(dscp.find('{0}description'.format(nmsp)).text)\n",
    "        dscript = dscp.find('{0}description'.format(nmsp)).text\n",
    "        dscript = re.split(r'<[^>]+>|[\\s\\n\\t]', dscript)\n",
    "        temp_info= np.array([ele for ele in dscript if len(ele)>0])\n",
    "        #print(len(temp_info))\n",
    "        if len(temp_info) == 26:\n",
    "            temp_info=np.append(temp_info,count)\n",
    "            timesteps=np.append(timesteps,count)\n",
    "            intensity=np.append(intensity,[temp_info[13]])\n",
    "            mslp=np.append(mslp,[temp_info[16]])\n",
    "            max_int=np.append(max_int,[temp_info[20]])\n",
    "            min_slp=np.append(min_slp,[temp_info[24]])\n",
    "            count+=1\n",
    "        elif len(temp_info) == 30:\n",
    "            timesteps=np.append(timesteps,count)\n",
    "            intensity=np.append(intensity,[temp_info[17]])\n",
    "            mslp=np.append(mslp,[temp_info[20]])\n",
    "            max_int=np.append(max_int,[temp_info[24]])\n",
    "            min_slp=np.append(min_slp,[temp_info[28]])\n",
    "            count+=1\n",
    "        elif len(temp_info) == 33:\n",
    "            timesteps=np.append(timesteps,count)\n",
    "            intensity=np.append(intensity,[temp_info[17]])\n",
    "            mslp=np.append(mslp,[temp_info[23]])\n",
    "            max_int=np.append(max_int,[temp_info[27]])\n",
    "            min_slp=np.append(min_slp,[temp_info[31]])\n",
    "            count+=1\n",
    "        elif len(temp_info) == 35:\n",
    "            timesteps=np.append(timesteps,count)\n",
    "            intensity=np.append(intensity,[temp_info[20]])\n",
    "            mslp=np.append(mslp,[temp_info[25]])\n",
    "            max_int=np.append(max_int,[temp_info[29]])\n",
    "            min_slp=np.append(min_slp,[temp_info[33]])\n",
    "            count+=1\n",
    "    for i in range(count,105):\n",
    "        timesteps=np.append(timesteps,[i], axis=0)\n",
    "        intensity= np.append(intensity, [0], axis = 0)\n",
    "        mslp= np.append(mslp, [0], axis = 0)\n",
    "        max_int= np.append(max_int, [0], axis = 0)\n",
    "        min_slp= np.append(min_slp, [0], axis = 0)\n",
    "        count+=1\n",
    "### Geogaphic coordinate feature extraction ###\n",
    "\n",
    "for fd in tree.iterfind('/{0}Document/{0}Folder/{0}Folder/{0}Folder'.format(nmsp)):\n",
    "    count=0\n",
    "    for coor in fd.iterfind('{0}Placemark/{0}Point'.format(nmsp)):\n",
    "        geocoor = coor.find('{0}coordinates'.format(nmsp)).text\n",
    "        lontitude,latitude,_ = geocoor.split(',')\n",
    "        lon = np.append(lon,lontitude)\n",
    "        lat = np.append(lat,latitude)\n",
    "        count+=1\n",
    "    for i in range(count,105):\n",
    "        lon = np.append(lon,lon[-1])\n",
    "        lat = np.append(lat,lat[-1])\n",
    "    #print(len(lon))\n",
    "lon = np.asanyarray(lon, dtype=np.float)\n",
    "lat = np.asanyarray(lat, dtype=np.float)\n",
    "intensity = np.asanyarray(intensity, dtype=np.float)\n",
    "mslp = np.asanyarray(mslp, dtype=np.float)\n",
    "max_int = np.asanyarray(max_int, dtype=np.float)\n",
    "min_slp = np.asanyarray(min_slp, dtype=np.float)\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    if lon[i] < 0:\n",
    "        lon[i] = lon[i]+360\n",
    "for i in range(len(mslp)):\n",
    "    if mslp[i] == 9999:\n",
    "        mslp[i] = mslp[np.where((mslp!=9999)|(mslp!=0))].mean()\n",
    "\n",
    "lon_train, lon_test = lon[:71505], lon[-23835:]\n",
    "lat_train, lat_test = lat[:71505], lat[-23835:]\n",
    "intensity_train, intensity_test = intensity[:71505], intensity[-23835:]\n",
    "mslp_train, mslp_test = mslp[:71505], mslp[-23835:]\n",
    "max_int_train, max_int_test = max_int[:71505], max_int[-23835:]\n",
    "min_slp_train, min_slp_test = min_slp[0:71505], min_slp[-23835:]\n",
    "\n",
    "#  Dataframe for training\n",
    "data_train = {'lon':lon_train,'lat':lat_train,'Intensity':intensity_train}\n",
    "data_test = {'lon':lon_test,'lat':lat_test,'Intensity':intensity_test}\n",
    "X_train = pd.DataFrame(data=data_train)\n",
    "X_test = pd.DataFrame(data=data_test)\n",
    "# target Dataframe\n",
    "y_train = X_train.shift(1)\n",
    "y_train = pd.DataFrame(data=y_train)\n",
    "y_train.fillna(0, inplace=True)\n",
    "y_test = X_test.shift(1)\n",
    "y_test = pd.DataFrame(data=y_test)\n",
    "y_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False, encoding='utf-8')\n",
    "X_test.to_csv('X_test.csv', index=False, encoding='utf-8')\n",
    "y_train.to_csv('y_train.csv', index=False, encoding='utf-8')\n",
    "y_test.to_csv('y_test.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71505 entries, 0 to 71504\n",
      "Data columns (total 3 columns):\n",
      "lon          71505 non-null float64\n",
      "lat          71505 non-null float64\n",
      "Intensity    71505 non-null float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 1.6 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('X_train.csv')\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
